# 统计学习基础

![Author](https://img.shields.io/badge/Author-AndrewYq-orange.svg)
![Email](https://img.shields.io/badge/Email-hfyqstar@163.com-blue.svg)

统计学习三要素关系可表示为：方法=模型+策略+算法。

## 模型

### 模型形式

统计学习首要考虑的问题就是学习什么样的模型。在监督学习中，模型就是所要学习的条件概率分布或决策函数。模型的假设空间（Hypothesis Space）包含所有可能的条件概率分布或决策函数。

* 条件概率分布

    ![条件概率分布](https://latex.codecogs.com/gif.latex?%5Cpounds%20%3D%20%5Cleft%20%5C%7B%20P%20%7C%20P%28Y%20%7C%20X%29%20%5Cright%20%5C%7D)

* 决策函数

    ![决策函数](https://latex.codecogs.com/gif.latex?%5Cpounds%20%3D%20%5Cleft%20%5C%7B%20f%20%7C%20Y%3Df%28x%29%20%5Cright%20%5C%7D)

### 模型评估与选择

统计学习的目的是使学到的模型不仅对于已知数据而且对于未知数据都能有很好的预测能力，不同的学习方法给出不同的学习模型。当损失函数给定时，基于损失函数的模型的训练误差（Training Error）和模型的测试误差（Test Error）就自然成为学习方法的评估标准，但统计学习方法采用的损失函数未必是评估时使用的损失函数。

假设学习到的模型是：![model](https://latex.codecogs.com/gif.latex?Y%20%3D%20%5Chat%20f%28X%29)，训练误差是该模型关于训练数据集的平均损失：

![training error](https://latex.codecogs.com/gif.latex?e_%7Btraining%7D%28%5Chat%20f%29%20%3D%20%5Cfrac%7B1%7D%7BN%7D%20%5Csum_%7Bi%3D1%7D%5E%7BN%7DL%28y_%7Bi%7D%2C%5Chat%20f%28x_%7Bi%7D%29%29)

其中N是训练样本容量。

测试误差是该模型关于测试集数据的平均损失：

![test error](https://latex.codecogs.com/gif.latex?e_%7Btest%7D%28%5Chat%20f%29%20%3D%20%5Cfrac%20%7B1%7D%7B%7BN%7D%27%7D%20%5Csum_%7Bi%3D1%7D%5E%7B%7BN%7D%27%7DL%28y_%7Bi%7D%2C%5Chat%20f%28x_%7Bi%7D%29%29)

其中N'是测试样本容量。

### 正则化与交叉验证

#### 正则化

模型选择的典型方法是正则化（Regularization）。正则化是结构风险最小化策略的实现，是在经验风险上加一个正则化项（Regularizer）或罚项(Penalty Term)。正则化项一般是模型复杂度的单调递增函数，模型越复杂，正则化项值就越大。比如，正则化项可以是模型参数向量的范数。

正则化项可以选取不同的形式。例如，回归问题中，损失函数的平方损失，正则化项可以是参数向量的L2范数。(注：[范数](https://blog.csdn.net/a493823882/article/details/80569888))

![L\left ( w \right ) = \frac{1}{N} \sum_{i=1}^{N} (f(x_{i};w)-y_{i})^{2}+\frac{\lambda}{2} ||w^{2}||_{2}](https://latex.codecogs.com/gif.latex?L%5Cleft%20%28%20w%20%5Cright%20%29%20%3D%20%5Cfrac%7B1%7D%7BN%7D%20%5Csum_%7Bi%3D1%7D%5E%7BN%7D%20%28f%28x_%7Bi%7D%3Bw%29-y_%7Bi%7D%29%5E%7B2%7D&plus;%5Cfrac%7B%5Clambda%7D%7B2%7D%20%7C%7Cw%5E%7B2%7D%7C%7C_%7B2%7D)

其中||w||表示向量w的L2范数。正则化项也可以是参数向量的L1范数：

![L\left ( w \right ) = \frac{1}{N} \sum_{i=1}^{N} (f(x_{i};w)-y_{i})^{2}+\lambda ||w||_{1}](https://latex.codecogs.com/gif.latex?L%5Cleft%20%28%20w%20%5Cright%20%29%20%3D%20%5Cfrac%7B1%7D%7BN%7D%20%5Csum_%7Bi%3D1%7D%5E%7BN%7D%20%28f%28x_%7Bi%7D%3Bw%29-y_%7Bi%7D%29%5E%7B2%7D&plus;%5Clambda%20%7C%7Cw%7C%7C_%7B1%7D)

正则化符合奥卡姆剃刀原理（[Occam's Razor](https://baike.baidu.com/item/%E5%A5%A5%E5%8D%A1%E5%A7%86%E5%89%83%E5%88%80%E5%8E%9F%E7%90%86/10900565?fromtitle=%E5%A5%A5%E5%8D%A1%E5%A7%86%E5%89%83%E5%88%80&fromid=1673264&fr=aladdin)）。奥卡姆剃刀原理应用于模型选择时变为以下想法：在所有可能的模型中，能够很好地解释已知数据并且十分简单才是最好的模型，也就是应该选择的模型。


#### 交叉验证

另一种常用的模型选择方法是交叉验证。常见的交叉验证方法有：

* 简单交叉验证

* K折交叉验证

* 留一交叉验证




## 策略

损失函数（Loss Function）或代价函数(Cost Function)：

* 损失函数：

    * 0-1损失函数（0-1 loss function）

        ![first equation](https://latex.codecogs.com/gif.latex?%5Cdpi%7B80%7D%20L%28Y%2Cf%28X%29%29%3D%5Cbegin%7Bcases%7D%201%2C%20%26%20Y%20%5Cneq%20f%28X%29%20%5C%5C%200%2C%20%26%20Y%20%3D%20f%28X%29%20%5Cend%7Bcases%7D)

    * 平方损失函数（quadratic loss function）

        ![second_equation](https://latex.codecogs.com/gif.latex?%5Cdpi%7B80%7D%20L%28Y%2Cf%28X%29%29%3D%28Y-f%28X%29%29%5E%7B2%7D)

    * 绝对损失函数（absolute loss function）

        ![third_equation](https://latex.codecogs.com/gif.latex?%5Cdpi%7B80%7D%20L%28Y%2Cf%28X%29%29%3D%5Cleft%20%7C%20Y-f%28X%29%20%5Cright%20%7C)

    * 对数损失函数（logarithmic loss function）或对数似然损失函数（log-likelihood loss function）

        ![fourth_equation](https://latex.codecogs.com/gif.latex?%5Cdpi%7B80%7D%20L%28Y%2CP%28Y%7CX%29%29%3D-logP%28Y%7CX%29)

* 风险函数(risk function)或期望损失函数（expect loss function）：

    ![risk function](https://latex.codecogs.com/gif.latex?%5Cdpi%7B80%7D%20R_%7Bexp%7D%3DE_%7Bp%7D%5BL%28Y%2CP%28Y%7CX%29%29%5D%3D%5Cint_%7Bx%5Ctimes%20y%7DL%28y%2Cf%28x%29%29P%28x%2Cy%29dxdy)

统计学习的目的就是选择期望风险最小的模型，但由于P(X,Y)是未知的，风险函数不能直接计算，所以监督学习成为一个病态问题（ill-formed problem）。

* 训练集的平均损失称为经验风险（empirical risk）或经验损失（empirical loss）:

    ![empirical risk](https://latex.codecogs.com/gif.latex?%5Cdpi%7B80%7D%20R_%7Bemp%7D%28f%29%3D%5Cfrac%7B1%7D%7BN%7D%5Csum_%7Bi%3D1%7D%5E%7BN%7DL%28y_%7Bi%7D%2Cf%28x_%7Bi%7D%29%29)

期望风险是模型关于联合分布的期望损失，经验风险是模型关于训练样本的平均损失。根据大数定律，当样本量N趋于无穷时，经验风险趋于期望风险。由于现实中训练样本数量有限，所以用经验风险估计期望风险常常不是很理想，要对经验风险进行一定的矫正。这就关系到监督学习的两个基本策略：经验风险最小化和结构风险最小化。

* 结构风险(structural risk)

    ![structural risk](https://latex.codecogs.com/gif.latex?R_%7Bsrm%7D%28f%29%3D%5Cfrac%7B1%7D%7BN%7D%5Csum_%7Bi%3D1%7D%5E%7BN%7DL%28y_%7Bi%7D%2Cf%28x_%7Bi%7D%29%29&plus;%5Clambda%20J%28f%29)

    J(f)为模型的复杂度，模型f越复杂，复杂度J(f)就越大；反之，模型f越简单，复杂度J(f)就越小。也就是说，模型的复杂度表示了对复杂模型的惩罚。λ≥0是系数，用以权衡经验风险和模型复杂度。结构风险小需要经验风险与模型复杂度同时小。结构风险小的模型往往对于训练数据及测试数据都具有较好的预测。

    这样监督学习问题就变成了经验风险最小的模型是最优的模型，所以求最优模型就是求解最优化问题。这时的经验或结构风险函数是最优化的目标函数。

## 算法

算法是指学习模型的具体计算方法。统计学习基于训练数据集，根据学习策略，从假设空间中选择最优模型，最后考虑用什么样的计算方法求解最优模型。

这时，统计学习问题归结为最优化问题，统计学习的算法成为求解最优化问题的算法。如果优化问题有显示的解析解，这个最优化问题就比较简单。但通常解析解不存在，这就需要用数值计算的方法求解。统计学习可以利用已有的最优化算法，有时也需要开发独自的最优化算法。
